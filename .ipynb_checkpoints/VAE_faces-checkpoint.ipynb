{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aec18ab",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbcb3219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images : 100000\n",
      "Found 100000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"/home/david/Asignaturas/MIA/DL/img_align_celeba_subset\"\n",
    "\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "#from keras.utils import plot_model\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "filenames = np.array(glob.glob(os.path.join(DATA_FOLDER, '*/*.jpg')))\n",
    "NUM_IMAGES = len(filenames)\n",
    "print(\"Total number of images : \" + str(NUM_IMAGES))\n",
    "# prints : Total number of images : 100000\n",
    "\n",
    "INPUT_DIM = (128,128,1) # Image dimension\n",
    "BATCH_SIZE = 512\n",
    "Z_DIM = 200 # Dimension of the latent vector (z)\n",
    "\n",
    "\n",
    "data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(DATA_FOLDER, \n",
    "    target_size = INPUT_DIM[:2],\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    class_mode = 'input',\n",
    "    subset = 'training',\n",
    "    color_mode='grayscale'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf1a18",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ab646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " encoder_conv_0 (Conv2D)        (None, 64, 64, 16)   160         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 16)   0           ['encoder_conv_0[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_1 (Conv2D)        (None, 32, 32, 32)   4640        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 32, 32, 32)   0           ['encoder_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_2 (Conv2D)        (None, 16, 16, 32)   9248        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 16, 16, 32)   0           ['encoder_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_3 (Conv2D)        (None, 8, 8, 32)     9248        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 8, 8, 32)     0           ['encoder_conv_3[0][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 200)          409800      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " log_var (Dense)                (None, 200)          409800      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)        (None, 200)          0           ['mu[0][0]',                     \n",
      "                                                                  'log_var[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 842,896\n",
      "Trainable params: 842,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    " \n",
    "# ENCODER\n",
    "def build_vae_encoder(input_dim, output_dim, conv_filters, conv_kernel_size, conv_strides):\n",
    "  \n",
    "    # Clear tensorflow session to reset layer index numbers to 0 for LeakyRelu, \n",
    "    # BatchNormalization and Dropout.\n",
    "    # Otherwise, the names of above mentioned layers in the model \n",
    "    # would be inconsistent\n",
    "    global K\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Number of Conv layers\n",
    "    n_layers = len(conv_filters)\n",
    "\n",
    "    # Define model input\n",
    "    encoder_input = Input(shape = input_dim, name = 'encoder_input')\n",
    "    x = encoder_input\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(n_layers):\n",
    "        x = Conv2D(filters = conv_filters[i], \n",
    "            kernel_size = conv_kernel_size[i],\n",
    "            strides = conv_strides[i], \n",
    "            padding = 'same',\n",
    "            name = 'encoder_conv_' + str(i)\n",
    "            )(x)\n",
    "\n",
    "        x = LeakyReLU()(x)\n",
    "        \n",
    "    # Required for reshaping latent vector while building Decoder\n",
    "    shape_before_flattening = K.int_shape(x)[1:] \n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    mean_mu = Dense(output_dim, name = 'mu')(x)\n",
    "    log_var = Dense(output_dim, name = 'log_var')(x)\n",
    "\n",
    "    # Defining a function for sampling\n",
    "    def sampling(args):\n",
    "        mean_mu, log_var = args\n",
    "        epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n",
    "        return mean_mu + K.exp(log_var/2)*epsilon   \n",
    "\n",
    "    # Using a Keras Lambda Layer to include the sampling function as a layer \n",
    "    # in the model\n",
    "    encoder_output = Lambda(sampling, name='encoder_output')([mean_mu, log_var])\n",
    "\n",
    "\n",
    "    return encoder_input, encoder_output, mean_mu, log_var, shape_before_flattening, Model(encoder_input, encoder_output)\n",
    "\n",
    "\n",
    "vae_encoder_input, vae_encoder_output,  mean_mu, log_var, vae_shape_before_flattening, vae_encoder  = build_vae_encoder(\n",
    "    input_dim = INPUT_DIM,\n",
    "    output_dim = Z_DIM, \n",
    "    #conv_filters = [32, 64, 64, 64],\n",
    "    conv_filters = [16, 32, 32, 32],\n",
    "    conv_kernel_size = [3,3,3,3],\n",
    "    conv_strides = [2,2,2,2])\n",
    "\n",
    "vae_encoder.summary()\n",
    "\n",
    "plot_model(vae_encoder, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d97cc",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feab64d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 200)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              411648    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " decoder_conv_0 (Conv2DTrans  (None, 16, 16, 32)       9248      \n",
      " pose)                                                           \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " decoder_conv_1 (Conv2DTrans  (None, 32, 32, 32)       9248      \n",
      " pose)                                                           \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " decoder_conv_2 (Conv2DTrans  (None, 64, 64, 16)       4624      \n",
      " pose)                                                           \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 64, 64, 16)        0         \n",
      "                                                                 \n",
      " decoder_conv_3 (Conv2DTrans  (None, 128, 128, 1)      145       \n",
      " pose)                                                           \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128, 128, 1)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 434,913\n",
      "Trainable params: 434,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_decoder(input_dim, shape_before_flattening, conv_filters, conv_kernel_size, conv_strides):\n",
    "\n",
    "    # Number of Conv layers\n",
    "    n_layers = len(conv_filters)\n",
    "\n",
    "    # Define model input\n",
    "    decoder_input = Input(shape = (input_dim,) , name = 'decoder_input')\n",
    "\n",
    "    # To get an exact mirror image of the encoder\n",
    "    x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "    x = Reshape(shape_before_flattening)(x)\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(n_layers):\n",
    "        x = Conv2DTranspose(\n",
    "            filters = conv_filters[i], \n",
    "            kernel_size = conv_kernel_size[i],\n",
    "            strides = conv_strides[i], \n",
    "            padding = 'same',\n",
    "            name = 'decoder_conv_' + str(i)\n",
    "            )(x)\n",
    "        \n",
    "        # Adding a sigmoid layer at the end to restrict the outputs \n",
    "        # between 0 and 1\n",
    "        if i < n_layers - 1:\n",
    "            x = LeakyReLU()(x)\n",
    "        else:\n",
    "            x = Activation('sigmoid')(x)\n",
    "\n",
    "    # Define model output\n",
    "    decoder_output = x\n",
    "\n",
    "    return decoder_input, decoder_output, Model(decoder_input, decoder_output)\n",
    "\n",
    "decoder_input, decoder_output, vae_decoder = build_decoder(input_dim = Z_DIM,\n",
    "    shape_before_flattening = vae_shape_before_flattening,\n",
    "    #conv_filters = [64,64,32,3],        \n",
    "    conv_filters = [32,32,16,1],\n",
    "    conv_kernel_size = [3,3,3,3],\n",
    "    conv_strides = [2,2,2,2]\n",
    "    )\n",
    "\n",
    "vae_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702949c4",
   "metadata": {},
   "source": [
    "### Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e944e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " encoder_conv_0 (Conv2D)        (None, 64, 64, 16)   160         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 16)   0           ['encoder_conv_0[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_1 (Conv2D)        (None, 32, 32, 32)   4640        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 32, 32, 32)   0           ['encoder_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_2 (Conv2D)        (None, 16, 16, 32)   9248        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 16, 16, 32)   0           ['encoder_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_3 (Conv2D)        (None, 8, 8, 32)     9248        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 8, 8, 32)     0           ['encoder_conv_3[0][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 200)          409800      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " log_var (Dense)                (None, 200)          409800      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)        (None, 200)          0           ['mu[0][0]',                     \n",
      "                                                                  'log_var[0][0]']                \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 128, 128, 1)  434913      ['encoder_output[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,277,809\n",
      "Trainable params: 1,277,809\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae_input = vae_encoder_input\n",
    "vae_output = vae_decoder(vae_encoder_output)\n",
    "\n",
    "# Input to the combined model will be the input to the encoder.\n",
    "# Output of the combined model will be the output of the decoder.\n",
    "vae = Model(vae_input, vae_output)\n",
    "\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5336a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reconstructing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a33b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_compare_VAE(images, add_noise=False):\n",
    "    \n",
    "    n_to_show = images.shape[0]\n",
    "\n",
    "    if add_noise:\n",
    "        encodings = VAE_encoder.predict(images)\n",
    "        encodings += np.random.normal(0.0, 1.0, size = (n_to_show,200))\n",
    "        reconst_images = VAE_decoder.predict(encodings)\n",
    "\n",
    "    else:\n",
    "        reconst_images = vae.predict(images)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    fig.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.95, hspace=0.1, wspace=0.1)\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        #img = images[i].squeeze()\n",
    "        img = images[i]\n",
    "        sub = fig.add_subplot(2, n_to_show, i+1)\n",
    "        sub.axis('off')        \n",
    "        sub.imshow(img,cmap='gray')\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        img = reconst_images[i].squeeze()\n",
    "        sub = fig.add_subplot(2, n_to_show, i+n_to_show+1)\n",
    "        sub.axis('off')\n",
    "        sub.imshow(img,cmap='gray')\n",
    "    plt.show()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada9f60",
   "metadata": {},
   "source": [
    "### Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03067dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:38:45.019945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.075295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.076902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.079885: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 20:38:45.081675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.084088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.085722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.928883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.929288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.929604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-18 20:38:45.929909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9489 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 20:38:47.172377: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8100\n",
      "2023-04-18 20:38:48.062839: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-18 20:38:48.063200: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-18 20:38:48.063216: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-04-18 20:38:48.063544: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-18 20:38:48.063603: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 61s 297ms/step - batch: 97.5000 - size: 510.2041 - loss: 476.5943 - r_loss: 0.0439 - kl_loss: 36.1381\n",
      "Epoch 2/2\n",
      "  8/195 [>.............................] - ETA: 58s - batch: 3.5000 - size: 512.0000 - loss: 292.4676 - r_loss: 0.0238 - kl_loss: 54.9454"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "N_EPOCHS = 2  # No. of epochs to show advance\n",
    "N_BLOCKS = 10\n",
    "LOSS_FACTOR = 10000\n",
    "\n",
    "adam_optimizer = Adam(learning_rate = LEARNING_RATE)\n",
    "\n",
    "def r_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "\n",
    "def kl_loss(y_true, y_pred):\n",
    "    kl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean_mu) - K.exp(log_var), axis = 1)\n",
    "    return kl_loss\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    return LOSS_FACTOR*r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "vae.compile(optimizer=adam_optimizer, loss = total_loss, metrics = [r_loss, kl_loss])\n",
    "\n",
    "\n",
    "example_batch = next(data_flow)\n",
    "example_batch = example_batch[0]\n",
    "example_images = example_batch[:8]\n",
    "\n",
    "for i in range(N_BLOCKS):\n",
    "    vae.fit(data_flow, \n",
    "        shuffle=True, \n",
    "        epochs = N_EPOCHS, \n",
    "        initial_epoch = 0, \n",
    "        steps_per_epoch=NUM_IMAGES / BATCH_SIZE)\n",
    "    plot_compare_VAE(example_images) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946e471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
