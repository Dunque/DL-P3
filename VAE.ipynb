{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34d4d5a7",
   "metadata": {},
   "source": [
    "# DEEP LEARNING - PRACTICUM 03\n",
    "\n",
    "- Diego Roca Rodríguez\n",
    "- Roi Santos Ríos\n",
    "\n",
    "Variational Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22392daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:58:37.807252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "from numpy import iscomplexobj\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "from utils import display, sample_batch, display_one, display_list\n",
    "\n",
    "disable_eager_execution()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8d9bd26",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18715577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images : 202599\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"./data/img_align_celeba/\"\n",
    "\n",
    "filenames = np.array(glob.glob(\"./data/img_align_celeba/*.jpg\"))\n",
    "NUM_IMAGES = len(filenames)\n",
    "print(\"Total number of images : \" + str(NUM_IMAGES))\n",
    "# prints : Total number of images : 100000\n",
    "\n",
    "INPUT_DIM = (16,16,1) # Image dimension\n",
    "BATCH_SIZE = 128 # Batch size\n",
    "Z_DIM = 128 # Dimension of the latent vector (z)\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "N_EPOCHS = 2  # No. of epochs to show advance\n",
    "N_BLOCKS = 10\n",
    "LOSS_FACTOR = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5aec18ab",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcb3219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(DATA_FOLDER, \n",
    "    target_size = INPUT_DIM[:2],\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    class_mode = 'input',\n",
    "    subset = 'training',\n",
    "    color_mode='grayscale'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d13186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Inception v3 model\n",
    "inception_v3 = hub.KerasLayer('https://tfhub.dev/google/imagenet/inception_v3/feature_vector/5', input_shape=(299, 299, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79cf1a18",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ab646a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 16, 16, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " encoder_conv_0 (Conv2D)        (None, 8, 8, 16)     160         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 8, 8, 16)     0           ['encoder_conv_0[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_1 (Conv2D)        (None, 4, 4, 32)     4640        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 4, 4, 32)     0           ['encoder_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_2 (Conv2D)        (None, 2, 2, 32)     9248        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 2, 2, 32)     0           ['encoder_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 128)          0           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 128)          16512       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " log_var (Dense)                (None, 128)          16512       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)        (None, 128)          0           ['mu[0][0]',                     \n",
      "                                                                  'log_var[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47,072\n",
      "Trainable params: 47,072\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# ENCODER\n",
    "def build_vae_encoder(input_dim, output_dim, conv_filters, conv_kernel_size, conv_strides):\n",
    "  \n",
    "    # Clear tensorflow session to reset layer index numbers to 0 for LeakyRelu, \n",
    "    # BatchNormalization and Dropout.\n",
    "    # Otherwise, the names of above mentioned layers in the model \n",
    "    # would be inconsistent\n",
    "    global K\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Number of Conv layers\n",
    "    n_layers = len(conv_filters)\n",
    "\n",
    "    # Define model input\n",
    "    encoder_input = Input(shape = input_dim, name = 'encoder_input')\n",
    "    x = encoder_input\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(n_layers):\n",
    "        x = Conv2D(filters = conv_filters[i], \n",
    "            kernel_size = conv_kernel_size[i],\n",
    "            strides = conv_strides[i], \n",
    "            padding = 'same',\n",
    "            name = 'encoder_conv_' + str(i)\n",
    "            )(x)\n",
    "\n",
    "        x = LeakyReLU()(x)\n",
    "        \n",
    "    # Required for reshaping latent vector while building Decoder\n",
    "    shape_before_flattening = K.int_shape(x)[1:] \n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    mean_mu = Dense(output_dim, name = 'mu')(x)\n",
    "    log_var = Dense(output_dim, name = 'log_var')(x)\n",
    "\n",
    "    # Defining a function for sampling\n",
    "    def sampling(args):\n",
    "        mean_mu, log_var = args\n",
    "        epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.) \n",
    "        return mean_mu + K.exp(log_var/2)*epsilon   \n",
    "\n",
    "    # Using a Keras Lambda Layer to include the sampling function as a layer \n",
    "    # in the model\n",
    "    encoder_output = Lambda(sampling, name='encoder_output')([mean_mu, log_var])\n",
    "\n",
    "\n",
    "    return encoder_input, encoder_output, mean_mu, log_var, shape_before_flattening, Model(encoder_input, encoder_output)\n",
    "\n",
    "\n",
    "vae_encoder_input, vae_encoder_output,  mean_mu, log_var, vae_shape_before_flattening, vae_encoder  = build_vae_encoder(\n",
    "    input_dim = INPUT_DIM,\n",
    "    output_dim = Z_DIM, \n",
    "    conv_filters = [16, 32, 32],\n",
    "    conv_kernel_size = [3,3,3],\n",
    "    conv_strides = [2,2,2])\n",
    "\n",
    "vae_encoder.summary()\n",
    "\n",
    "plot_model(vae_encoder, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da4d97cc",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feab64d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 2, 2, 32)          0         \n",
      "                                                                 \n",
      " decoder_conv_0 (Conv2DTrans  (None, 4, 4, 32)         9248      \n",
      " pose)                                                           \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " decoder_conv_1 (Conv2DTrans  (None, 8, 8, 16)         4624      \n",
      " pose)                                                           \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 8, 8, 16)          0         \n",
      "                                                                 \n",
      " decoder_conv_2 (Conv2DTrans  (None, 16, 16, 1)        145       \n",
      " pose)                                                           \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 16, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,529\n",
      "Trainable params: 30,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_decoder(input_dim, shape_before_flattening, conv_filters, conv_kernel_size, conv_strides):\n",
    "\n",
    "    # Number of Conv layers\n",
    "    n_layers = len(conv_filters)\n",
    "\n",
    "    # Define model input\n",
    "    decoder_input = Input(shape = (input_dim,) , name = 'decoder_input')\n",
    "\n",
    "    # To get an exact mirror image of the encoder\n",
    "    x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
    "    x = Reshape(shape_before_flattening)(x)\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i in range(n_layers):\n",
    "        x = Conv2DTranspose(\n",
    "            filters = conv_filters[i], \n",
    "            kernel_size = conv_kernel_size[i],\n",
    "            strides = conv_strides[i], \n",
    "            padding = 'same',\n",
    "            name = 'decoder_conv_' + str(i)\n",
    "            )(x)\n",
    "        \n",
    "        # Adding a sigmoid layer at the end to restrict the outputs \n",
    "        # between 0 and 1\n",
    "        if i < n_layers - 1:\n",
    "            x = LeakyReLU()(x)\n",
    "        else:\n",
    "            x = Activation('sigmoid')(x)\n",
    "\n",
    "    # Define model output\n",
    "    decoder_output = x\n",
    "\n",
    "    return decoder_input, decoder_output, Model(decoder_input, decoder_output)\n",
    "\n",
    "decoder_input, decoder_output, vae_decoder = build_decoder(input_dim = Z_DIM,\n",
    "    shape_before_flattening = vae_shape_before_flattening,        \n",
    "    conv_filters = [32,16,1],\n",
    "    conv_kernel_size = [3,3,3],\n",
    "    conv_strides = [2,2,2]\n",
    "    )\n",
    "\n",
    "vae_decoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "702949c4",
   "metadata": {},
   "source": [
    "### Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e944e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 16, 16, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " encoder_conv_0 (Conv2D)        (None, 8, 8, 16)     160         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 8, 8, 16)     0           ['encoder_conv_0[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_1 (Conv2D)        (None, 4, 4, 32)     4640        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 4, 4, 32)     0           ['encoder_conv_1[0][0]']         \n",
      "                                                                                                  \n",
      " encoder_conv_2 (Conv2D)        (None, 2, 2, 32)     9248        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 2, 2, 32)     0           ['encoder_conv_2[0][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 128)          0           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " mu (Dense)                     (None, 128)          16512       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " log_var (Dense)                (None, 128)          16512       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " encoder_output (Lambda)        (None, 128)          0           ['mu[0][0]',                     \n",
      "                                                                  'log_var[0][0]']                \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 16, 16, 1)    30529       ['encoder_output[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 77,601\n",
      "Trainable params: 77,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae_input = vae_encoder_input\n",
    "vae_output = vae_decoder(vae_encoder_output)\n",
    "\n",
    "# Input to the combined model will be the input to the encoder.\n",
    "# Output of the combined model will be the output of the decoder.\n",
    "vae = Model(vae_input, vae_output)\n",
    "\n",
    "vae.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5336a28",
   "metadata": {},
   "source": [
    "### Reconstructing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a33b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_VAE(images, add_noise=False):\n",
    "    \n",
    "    n_to_show = images.shape[0]\n",
    "\n",
    "    if add_noise:\n",
    "        encodings = vae_encoder.predict(images)\n",
    "        encodings += np.random.normal(0.0, 1.0, size = (n_to_show,200))\n",
    "        reconst_images = vae_decoder.predict(encodings)\n",
    "\n",
    "    else:\n",
    "        reconst_images = vae.predict(images)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    fig.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.95, hspace=0.1, wspace=0.1)\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        #img = images[i].squeeze()\n",
    "        img = images[i]\n",
    "        sub = fig.add_subplot(2, n_to_show, i+1)\n",
    "        sub.axis('off')        \n",
    "        sub.imshow(img,cmap='gray')\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        img = reconst_images[i].squeeze()\n",
    "        sub = fig.add_subplot(2, n_to_show, i+n_to_show+1)\n",
    "        sub.axis('off')\n",
    "        sub.imshow(img,cmap='gray')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateFid(callbacks.Callback):\n",
    "    def __init__(self, num_img, latent_dim):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    # Define a function to calculate FID\n",
    "    def calculate_fid(self, real_features, generated_features):\n",
    "        # Calculate the mean and covariance of the real and generated features\n",
    "        mu_real = np.mean(real_features, axis=0)\n",
    "        mu_generated = np.mean(generated_features, axis=0)\n",
    "        cov_real = np.cov(real_features, rowvar=False)\n",
    "        cov_generated = np.cov(generated_features, rowvar=False)\n",
    "\n",
    "        # Calculate the squared difference between the means\n",
    "        diff = mu_real - mu_generated\n",
    "        diff_squared = np.dot(diff, diff)\n",
    "\n",
    "        # Calculate the square root of the product of the covariances\n",
    "        cov_sqrt = sqrtm(cov_real.dot(cov_generated))\n",
    "        \n",
    "        if iscomplexobj(cov_sqrt):\n",
    "            cov_sqrt = cov_sqrt.real\n",
    "\n",
    "        # Calculate the FID\n",
    "        fid = diff_squared + np.trace(cov_real + cov_generated - 2 * cov_sqrt)\n",
    "\n",
    "        return fid\n",
    "\n",
    "    def preprocess_inception(self, image):\n",
    "        image = tf.image.resize(image, (299, 299))\n",
    "        image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
    "        return image\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        real_images = np.array([tf.keras.preprocessing.image.img_to_array(\n",
    "            tf.keras.preprocessing.image.load_img(image_path, target_size=(32, 32)))\n",
    "                                for image_path in np.random.choice(\n",
    "                                    glob.glob(\"./data/img_align_celeba/*.jpg\"), self.num_img)])\n",
    "\n",
    "        # Generate a batch of images using the model\n",
    "        generated_images = vae.predict(real_images)\n",
    "\n",
    "        # Preprocess the images\n",
    "        prepd_gen_images = np.array([self.preprocess_inception(image) for image in generated_images])\n",
    "\n",
    "        # Extract features from the images\n",
    "        generated_features = inception_v3(prepd_gen_images)\n",
    "        generated_features = tf.keras.backend.eval(generated_features)\n",
    "\n",
    "        print(\"GENERATED IMAGES, EPOCH: \", epoch)\n",
    "        display_list(generated_images, cmap=\"None\")\n",
    "\n",
    "        # Preprocess the images\n",
    "        prepd_real_images = np.array([self.preprocess_inception(image) for image in real_images])\n",
    "\n",
    "        # Extract features from the images\n",
    "        real_features = inception_v3(prepd_real_images)\n",
    "        real_features = tf.keras.backend.eval(real_features)\n",
    "\n",
    "        #print(real_features.shape)\n",
    "        print(\"REAL IMAGES, EPOCH: \", epoch)\n",
    "        display_list(real_images, cmap=\"None\")\n",
    "        \n",
    "        # Calculate the FID\n",
    "        fid = self.calculate_fid(real_features, generated_features)\n",
    "        print(\"-------------------------\")\n",
    "        print(\"EPOCH: \", epoch, \" FID: \", fid)\n",
    "        \n",
    "calculate_fid = CalculateFid(num_img=10, latent_dim=Z_DIM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fada9f60",
   "metadata": {},
   "source": [
    "### Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03067dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:58:40.659010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 12:58:40.659883: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-05-12 12:58:40.667195: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-05-12 12:58:40.683516: W tensorflow/c/c_api.cc:291] Operation '{name:'mu/bias/Assign' id:96 op device:{requested: '', assigned: ''} def:{{{node mu/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](mu/bias, mu/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "adam_optimizer = Adam(learning_rate = LEARNING_RATE)\n",
    "\n",
    "def r_loss(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
    "\n",
    "def kl_loss(y_true, y_pred):\n",
    "    kl_loss =  -0.5 * K.sum(1 + log_var - K.square(mean_mu) - K.exp(log_var), axis = 1)\n",
    "    return kl_loss\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    return LOSS_FACTOR*r_loss(y_true, y_pred) + kl_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "vae.compile(optimizer=adam_optimizer, loss = total_loss, metrics = [r_loss, kl_loss])\n",
    "\n",
    "\n",
    "example_batch = next(data_flow)\n",
    "example_batch = example_batch[0]\n",
    "example_images = example_batch[:8]\n",
    "\n",
    "for i in range(N_BLOCKS):\n",
    "    vae.fit(\n",
    "        data_flow, \n",
    "        shuffle=True, \n",
    "        epochs = N_EPOCHS, \n",
    "        initial_epoch = 0, \n",
    "        steps_per_epoch=NUM_IMAGES / BATCH_SIZE,\n",
    "        callbacks=[calculate_fid])\n",
    "    plot_compare_VAE(example_images) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
